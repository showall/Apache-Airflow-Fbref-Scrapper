[2023-04-23T17:43:09.338+0800] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: scrapy_fbref_players.run_spider scheduled__2023-04-21T17:05:00+00:00 [queued]>
[2023-04-23T17:43:09.343+0800] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: scrapy_fbref_players.run_spider scheduled__2023-04-21T17:05:00+00:00 [queued]>
[2023-04-23T17:43:09.343+0800] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T17:43:09.344+0800] {taskinstance.py:1289} INFO - Starting attempt 1 of 2
[2023-04-23T17:43:09.344+0800] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T17:43:09.360+0800] {taskinstance.py:1309} INFO - Executing <Task(PythonOperator): run_spider> on 2023-04-21 17:05:00+00:00
[2023-04-23T17:43:09.363+0800] {standard_task_runner.py:55} INFO - Started process 1323 to run task
[2023-04-23T17:43:09.367+0800] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'scrapy_fbref_players', 'run_spider', 'scheduled__2023-04-21T17:05:00+00:00', '--job-id', '21', '--raw', '--subdir', 'DAGS_FOLDER/scrapy_fbref_players_dag.py', '--cfg-path', '/tmp/tmp3iphmxwv']
[2023-04-23T17:43:09.369+0800] {standard_task_runner.py:83} INFO - Job 21: Subtask run_spider
[2023-04-23T17:43:09.413+0800] {task_command.py:389} INFO - Running <TaskInstance: scrapy_fbref_players.run_spider scheduled__2023-04-21T17:05:00+00:00 [running]> on host LAPTOP-3BHL50BT.localdomain
[2023-04-23T17:43:09.482+0800] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=scrapy_fbref_players
AIRFLOW_CTX_TASK_ID=run_spider
AIRFLOW_CTX_EXECUTION_DATE=2023-04-21T17:05:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-21T17:05:00+00:00
[2023-04-23T17:43:09.483+0800] {scrapy_fbref_players_dag.py:32} INFO - The first root directory is /home/gerardsho/airflow/dags/scrapyfbref.
[2023-04-23T17:45:03.294+0800] {process_utils.py:129} INFO - Sending Signals.SIGTERM to group 1323. PIDs of all processes in the group: [1324, 1323]
[2023-04-23T17:45:03.295+0800] {process_utils.py:84} INFO - Sending the signal Signals.SIGTERM to group 1323
[2023-04-23T17:45:03.297+0800] {taskinstance.py:1488} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-04-23T17:45:03.599+0800] {taskinstance.py:1776} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/gerardsho/airflow/venv/lib/python3.8/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/gerardsho/airflow/venv/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/gerardsho/airflow/dags/scrapy_fbref_players_dag.py", line 33, in run_spider
    call(["scrapy", "crawl", "fbref","-s","CLOSESPIDER_PAGECOUNT=10"])
  File "/usr/lib/python3.8/subprocess.py", line 342, in call
    return p.wait(timeout=timeout)
  File "/usr/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/usr/lib/python3.8/subprocess.py", line 1806, in _wait
    (pid, sts) = self._try_wait(0)
  File "/usr/lib/python3.8/subprocess.py", line 1764, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/home/gerardsho/airflow/venv/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1490, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-04-23T17:45:03.615+0800] {taskinstance.py:1327} INFO - Marking task as UP_FOR_RETRY. dag_id=scrapy_fbref_players, task_id=run_spider, execution_date=20230421T170500, start_date=20230423T094309, end_date=20230423T094503
[2023-04-23T17:45:03.664+0800] {standard_task_runner.py:100} ERROR - Failed to execute job 21 for task run_spider (Task received SIGTERM signal; 1323)
[2023-04-23T17:45:03.711+0800] {process_utils.py:79} INFO - Process psutil.Process(pid=1323, status='terminated', exitcode=1, started='17:43:08') (1323) terminated with exit code 1
[2023-04-23T17:45:03.712+0800] {process_utils.py:79} INFO - Process psutil.Process(pid=1324, status='terminated', started='17:43:08') (1324) terminated with exit code None
[2023-04-23T18:08:25.516+0800] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: scrapy_fbref_players.run_spider scheduled__2023-04-21T17:05:00+00:00 [queued]>
[2023-04-23T18:08:25.524+0800] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: scrapy_fbref_players.run_spider scheduled__2023-04-21T17:05:00+00:00 [queued]>
[2023-04-23T18:08:25.524+0800] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T18:08:25.524+0800] {taskinstance.py:1289} INFO - Starting attempt 1 of 2
[2023-04-23T18:08:25.524+0800] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T18:08:25.541+0800] {taskinstance.py:1309} INFO - Executing <Task(PythonOperator): run_spider> on 2023-04-21 17:05:00+00:00
[2023-04-23T18:08:25.544+0800] {standard_task_runner.py:55} INFO - Started process 1946 to run task
[2023-04-23T18:08:25.547+0800] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'scrapy_fbref_players', 'run_spider', 'scheduled__2023-04-21T17:05:00+00:00', '--job-id', '30', '--raw', '--subdir', 'DAGS_FOLDER/scrapy_fbref_players_dag.py', '--cfg-path', '/tmp/tmp95jtyssh']
[2023-04-23T18:08:25.549+0800] {standard_task_runner.py:83} INFO - Job 30: Subtask run_spider
[2023-04-23T18:08:25.598+0800] {task_command.py:389} INFO - Running <TaskInstance: scrapy_fbref_players.run_spider scheduled__2023-04-21T17:05:00+00:00 [running]> on host LAPTOP-3BHL50BT.localdomain
[2023-04-23T18:08:25.654+0800] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=scrapy_fbref_players
AIRFLOW_CTX_TASK_ID=run_spider
AIRFLOW_CTX_EXECUTION_DATE=2023-04-21T17:05:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-21T17:05:00+00:00
[2023-04-23T18:08:25.655+0800] {scrapy_fbref_players_dag.py:32} INFO - The first root directory is /home/gerardsho/airflow/dags/scrapyfbref.
[2023-04-23T18:10:35.853+0800] {scrapy_fbref_players_dag.py:34} INFO - The second root directory is /home/gerardsho/airflow/dags/scrapyfbref.
[2023-04-23T18:10:35.854+0800] {python.py:177} INFO - Done. Returned value was: None
[2023-04-23T18:10:35.860+0800] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=scrapy_fbref_players, task_id=run_spider, execution_date=20230421T170500, start_date=20230423T100825, end_date=20230423T101035
[2023-04-23T18:10:35.920+0800] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-23T18:10:35.931+0800] {taskinstance.py:2596} INFO - 1 downstream tasks scheduled from follow-on schedule check
